use_wandb: true
layerwise_logging: true
log_time: true
use_cuda: false
scale_observation: true
version_string: 'vanilla_v0'

dimension: 3
max_num_points: 20
max_value: 20  # Not sure if this impacts the game at all
max_grad_norm: 10

host:
  batch_size: 2
  initial_rollout_size: 2
  steps_before_rollout: 2
  steps_before_update_target: 2
  rollout_size: 2
  max_rollout_step: 10
  gradient_steps_per_loop: 10
  optim:
    name: 'adam'
    args: # Pass optimizer parameters here
      lr: 0.00000001
    lr_schedule: # (OPTIONAL) Use a scheduler on the learning rate
      mode: 'exponential'
      initial_lr: 0.001
      rate: 0.996
  er: 0.2  # Exploration rate
  er_schedule: # (OPTIONAL) Use a scheduler on the exploration rate
    mode: 'exponential'
    initial_er: 0.5
    rate: 0.996
  net_arch: [ 16, 'b', { repeat: 2, net_arch: [ 32, 'b' ] }, 16, 'b' ]
  gamma: 0.99
  tau: 0.9

agent:
  batch_size: 2
  initial_rollout_size: 2
  steps_before_rollout: 2
  steps_before_update_target: 2
  rollout_size: 2
  max_rollout_step: 10
  gradient_steps_per_loop: 10
  optim:
    name: 'adam'
    args:
      lr: 0.00000001
    lr_schedule:
      mode: 'exponential'
      initial_lr: 0.001
      rate: 0.996
  er: 0.2
  er_schedule:
    mode: 'exponential'
    initial_er: 0.5
    rate: 0.996
  net_arch: [ 16, 'b', { repeat: 2, net_arch: [ 32, 'b' ] }, 16, 'b' ]
  gamma: 0.99
  tau: 0.9

replay_buffer:
  type: 'base'
  buffer_size: 100
  use_cuda: false



